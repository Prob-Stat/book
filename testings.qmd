# 仮説検定

## 事前分布を仮定しない推定問題

ベイズ推定では推定したいパラメータ $\theta\in\Theta$ の事前分布 $\pi(\theta)$ を既知として仮定した。
しかし、現実の問題ではこの事前分布を適切に仮定する方法がない場合もある。
例えば

* 開発中の薬に効果があるかないか
* ある患者が病気かどうか
* サイコロに偏りがあるかどうか

といった問題について事前分布をどのように仮定するのが適切か不明瞭である。
そのような状況でデータ $x\in\mathcal{X}$ から パラメータ $\theta\in\Theta$ を推定する問題を考える。
尤度 $p(x\mid\theta)$ は既知とする。

## 伝統的な仮説検定

パラメータの集合を二つの部分集合 $\Theta_0$ と $\Theta_1$ に分割する。
つまり、$\Theta_0\cup \Theta_1 = \Theta$, $\Theta_0\cap \Theta_1=\varnothing$ である。
そしてパラメータが $\Theta_0$ に属するか $\Theta_1$ に属するかを知りたいとする。
このとき二つの命題
\begin{align*}
H_0\colon \theta\in\Theta_0\\
H_1\colon \theta\in\Theta_1
\end{align*}
を仮説という。
その二つの仮説のうちの**通常成り立っていると考える方**を $H_0$ とし(対応するパラメータ集合は $\Theta_0$) **帰無仮説**と呼ぶ。
また、そうでない方を $H_1$ とし(対応するパラメータ集合は $\Theta_1$) **対立仮説**と呼ぶ。
帰無仮説の例として

* 開発中の薬に効果はない
* ある患者が病気ではない
* サイコロに偏りはない

などがある。
それらに対応する対立仮説はそれぞれ

* 開発中の薬に効果がある
* ある患者が病気である
* サイコロに偏りがある

となる。
仮説検定の考え方では帰無仮説を棄却するかしないかを決める。
帰無仮説を棄却した場合、対立仮説を正しいと考え、帰無仮説を棄却しなかった場合は何も言えないと結論づける。

## 単純仮説

仮説 $\Theta_0$ と $\Theta_1$ がそれぞれ一元集合であるとき、$H_0$ と $H_1$ を**単純仮説**という。
$\Theta_0=\{\theta_0\}$, $\Theta_1=\{\theta_1\}$ として、 $p_0(x)\coloneqq p(x\mid \theta_0)$ と $p_1(x)\coloneqq p(x\mid \theta_1)$ とする。
データから仮説を推定する関数 $E\colon\mathcal{X}\to[0,1]$ を**検定関数**という。
各 $x\in\mathcal{X}$ について、$E(x)$ は**帰無仮説を棄却する確率**とする。
この検定関数について二種類の誤り確率を
\begin{align*}
\alpha_E &\coloneqq \expt{E(X)\mid \theta_0}=\sum_x E(x) p(x\mid \theta_0)\\
\beta_E &\coloneqq 1-\expt{E(X)\mid \theta_1}=1-\sum_x E(x) p(x\mid \theta_1)\\
\end{align*}
と定義する。
このとき、$\alpha_E$ は帰無仮説が正しいときに帰無仮説を棄却する確率であり、**第一種誤り確率**もしくは**有意水準**という。
また、$\beta_E$ は対立仮説が正しいときに帰無仮説を棄却しない確率であり、**第二種誤り確率**という。
第一種誤り確率だけを小さくしたければ $E(x) = 0$ とすればよいし、第二種誤り確率だけを小さくしたければ $E(x)=1$ とすればよい。

## 最強力検定

::: {#def-powerful}
検定関数 $E\colon\mathcal{X}\to[0,1]$ が有意水準 $\alpha\in[0,1]$ の**最強力検定** $\defiff$ $\alpha_E=\alpha$ であり、任意の $F\colon\mathcal{X}\to[0,1]$ について、$\alpha_F\le\alpha$ ならば $\beta_F\ge\beta$ が成り立つ。
:::

実現可能な誤り確率 $(\alpha,\,\beta)$ の集合
\begin{align*}
C &= \left\{(\alpha_E,\,\beta_E)\mid E\colon\mathcal{X}\to[0,1]\right\}
\end{align*}
について考える。
自明に $(1,0)$ と $(0,1)$ は実現可能である。

この集合 $C$ は凸集合である。
\begin{align*}
\alpha_{pE + (1-p)F} &= p\alpha_E + (1-p)\alpha_F\\
\beta_{pE + (1-p)F} &= p\beta_E + (1-p)\beta_F
\end{align*}
であることから、
\begin{align*}
\begin{pmatrix}
\alpha_{pE + (1-p)F},& \beta_{pE + (1-p)F}
\end{pmatrix}
&= p \begin{pmatrix}\alpha_E,&\beta_E\end{pmatrix} + (1-p) \begin{pmatrix}\alpha_F,&\beta_F\end{pmatrix}
\end{align*}
が確認できる。
また、検定結果を反転させた検定関数 $1-E(x)$ を考えると、
\begin{align*}
\begin{pmatrix}
\alpha_{1-E},& \beta_{1-E}
\end{pmatrix}&=
\begin{pmatrix}
1,&1
\end{pmatrix} -
\begin{pmatrix}
\alpha_{E},& \beta_{E}
\end{pmatrix}
\end{align*}
である。

まとめると、実現可能な $(\alpha,\,\beta)$ の集合 $C$ は

1. $(1,0),\,(0,1)\in C$
1. $C$ は 凸集合
1. $(\alpha,\,\beta)\in C\iff (1-\alpha,\,1-\beta)\in C$

を満たす。

```{python}
#| fig-width: 4
#| fig-height: 4
#| label: fig-between-curves
#| fig-cap: "実現可能な $(\\alpha,\\,\\beta)$ の集合の例"

import numpy as np
import matplotlib.pyplot as plt

# 区間 [0,1] でプロット
x = np.linspace(0, 1, 400)
y1 = 1-np.sqrt(1-(1-x)**2)
y2 = np.sqrt(1-x**2)

plt.figure()

# 2 本の曲線
plt.plot(x, y1)
plt.plot(x, y2)

# 間の領域を塗る
plt.fill_between(x, y1, y2, where=(y2 >= y1), alpha=0.3)

plt.xlim(0, 1)
plt.ylim(0, 1)
plt.xlabel("$\\alpha$")
plt.ylabel("$\\beta$")
plt.tight_layout()

ax = plt.gca()
ax.set_aspect("equal", adjustable="box")
```

各 $\alpha\in[0,1]$ について、$\beta$ を最小化するのが最強力検定であるので、この図の下のカーブが最強力検定で実現される $(\alpha,\,\beta)$ となる。

## 尤度比検定

ベイズ推定の枠組みではMAP推定量が誤り確率を最小化する推定量であった。
このMAP推定量は
\begin{align*}
E_\mathrm{MAP}(x) &=\begin{cases}
0&\text{if } \frac{p(x\mid\theta_0)}{p(x\mid\theta_1)}\ge \frac{\pi(\theta_1)}{\pi(\theta_0)}\\
1&\text{otherwise}
\end{cases}
\end{align*}
と表すことができる。

一般的に $\eta > 0,\,\kappa\in[0,1]$ について
\begin{align*}
E(x) &=\begin{cases}
0&\text{if } \frac{p(x\mid\theta_0)}{p(x\mid\theta_1)}> \eta\\
1&\text{if } \frac{p(x\mid\theta_0)}{p(x\mid\theta_1)}< \eta\\
\kappa&\text{otherwise}
\end{cases}
\end{align*}
という形の検定関数を尤度比検定という。

::: {#lem-np}
## ネイマン・ピアソンの補題

任意の尤度比検定は最強力検定である(逆も成り立つ)。
:::
::: {.proof}
尤度比検定 $E$ における尤度比の閾値を $\eta> 0$ とする。
任意の $F\colon \mathcal{X}\to[0,1]$ について
\begin{align*}
&(F(x) - E(x)) (p_0(x) - \eta p_1(x))\ge 0\qquad \forall x\in\mathcal{X}\\
\iff&F(x)p_0(x) - E(x)p_0(x) - \eta F(x) p_1(x) + \eta E(x) p_1(x)\ge 0\qquad \forall x\in\mathcal{X}\\
\implies&\alpha_F - \alpha_E - \eta (1-\beta_F)  + \eta (1-\beta_E)\ge 0\\
\iff&(\alpha_F - \alpha_E) + \eta(\beta_F-\beta_E)\ge 0.
\end{align*}
よって $\alpha_F\le\alpha_E$ ならば $\beta_F\ge\beta_E$ である。
:::

データ $\mathcal{X}$ が連続な場合は確率質量関数の代わりに確率密度関数を考える。

::: {#exm-binom}
コインを持っており、表が出る確率は $p_0$ か $p_1$ のどちらかである。
コインを独立に $N$ 回投げて表が出る確率が $p_0$ か $p_1$ かを推定したい。
$\mathcal{X}=\{0,1\}^N$ とし、0 は裏、1 は表に対応するものとする。
このとき尤度は
\begin{align*}
p(\mathbf{x}\mid p_k) &= \prod_{i=1}^N p_k^{x_i} (1-p_k)^{1-x_i}\qquad\text{for } k\in\{0,1\}
\end{align*}
である。
このとき尤度比は
\begin{align*}
\frac{p(\mathbf{x}\mid p_0)}{p(\mathbf{x}\mid p_1)} &= \frac{\prod_i p_0^{x_i}(1-p_0)^{1-x_i}}{\prod_i p_1^{x_i}(1-p_1)^{1-x_i}}\\
&= \left(\frac{p_0}{p_1}\right)^{\sum_i x_i}\left(\frac{1-p_0}{1-p_1}\right)^{N-\sum_i x_i}
\end{align*}
である。
よって尤度比は表が出た回数 $T(\mathbf{x})=\sum_i x_i$ から定まる。
なので $p_1> p_0$ とすると、表が出た回数が多いときに $E(\mathbf{x}) = 1$ とすることになる。
:::

